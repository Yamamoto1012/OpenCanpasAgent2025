"""
Sentiment Analyzer

ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ„Ÿæƒ…åˆ†æï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‹ONNX)
"""
import os
import re
import math
import statistics
from enum import Enum
from typing import Dict, Any, Tuple, Union, List, Optional

# spacyã¨ginzaã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¢ºèª
try:
    import spacy
    from spacy.language import Language
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    print("spacy/ginzaãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æ„Ÿæƒ…åˆ†ææ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™")

from config import settings, logger


class SentimentCategory(str, Enum):
    """æ„Ÿæƒ…åˆ†é¡ã‚«ãƒ†ã‚´ãƒª"""
    STRONG_POSITIVE = "strong_positive"
    MILD_POSITIVE = "mild_positive" 
    NEUTRAL = "neutral"
    MILD_NEGATIVE = "mild_negative"
    STRONG_NEGATIVE = "strong_negative"


class SentimentAnalyzer:
    """
    æ„Ÿæƒ…åˆ†æã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    
    ç’°å¢ƒå¤‰æ•°ã«åŸºã¥ã„ã¦ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¾ãŸã¯ãƒ¬ã‚¬ã‚·ãƒ¼å®Ÿè£…ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    - USE_HYBRID_SENTIMENT=true: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    - USE_HYBRID_SENTIMENT=false: ãƒ¬ã‚¬ã‚·ãƒ¼å®Ÿè£…(è¾æ›¸ãƒ™ãƒ¼ã‚¹)
    """
    
    def __init__(self):
        # è¨­å®šã«åŸºã¥ã„ã¦å®Ÿè£…ã‚’é¸æŠ
        use_hybrid = os.getenv('USE_HYBRID_SENTIMENT', 'true').lower() == 'true'
        
        if use_hybrid:
            logger.info("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ„Ÿæƒ…åˆ†æå™¨ã‚’ä½¿ç”¨")
            self._impl = self._create_hybrid_analyzer()
        else:
            logger.info("ãƒ¬ã‚¬ã‚·ãƒ¼æ„Ÿæƒ…åˆ†æå™¨ã‚’ä½¿ç”¨")
            self._impl = LegacySentimentAnalyzer()
    
    def _create_hybrid_analyzer(self):
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æå™¨ã‚’ä½œæˆ"""
        try:
            from .hybrid_analyzer import HybridSentimentAnalyzer
            
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
            confidence_threshold = float(os.getenv('SENTIMENT_CONFIDENCE_THRESHOLD', '0.7'))
            enable_onnx = os.getenv('ENABLE_ONNX_SENTIMENT', 'true').lower() == 'true'
            use_dummy_onnx = os.getenv('USE_DUMMY_ONNX', 'false').lower() == 'true'
            onnx_model_path = os.getenv('ONNX_MODEL_PATH')
            
            return HybridSentimentAnalyzer(
                confidence_threshold=confidence_threshold,
                enable_onnx=enable_onnx,
                onnx_model_path=onnx_model_path,
                use_dummy_onnx=use_dummy_onnx
            )
        except ImportError as e:
            logger.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æå™¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
            logger.info("ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†æå™¨ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
            return LegacySentimentAnalyzer()
        except Exception as e:
            logger.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æå™¨ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            logger.info("ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†æå™¨ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
            return LegacySentimentAnalyzer()
    
    def analyze(self, text: str) -> Tuple[float, SentimentCategory]:
        """
        æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œã™ã‚‹
        
        å¾Œæ–¹äº’æ›æ€§ã‚’ä¿ã¤ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        """
        try:
            if hasattr(self._impl, 'analyze'):
                result = self._impl.analyze(text)
                # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®å ´åˆã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
                if len(result) > 2:
                    return result[0], result[1]
                return result
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return 50.0, SentimentCategory.NEUTRAL
        except Exception as e:
            logger.error(f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return 50.0, SentimentCategory.NEUTRAL
    
    def analyze_with_metadata(self, text: str) -> Tuple[float, SentimentCategory, Dict[str, Any]]:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œã™ã‚‹
        
        æ–°ã—ã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å°‚ç”¨ï¼‰
        """
        if hasattr(self._impl, 'analyze') and hasattr(self._impl, 'get_metrics'):
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æå™¨
            return self._impl.analyze(text)
        else:
            # ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†æå™¨
            score, category = self._impl.analyze(text)
            metadata = {
                'method': 'legacy',
                'implementation': type(self._impl).__name__
            }
            return score, category, metadata
    
    def get_analyzer_info(self) -> Dict[str, Any]:
        """åˆ†æå™¨ã®æƒ…å ±ã‚’å–å¾—"""
        info = {
            'implementation': type(self._impl).__name__,
            'version': '2.0.0'
        }
        
        if hasattr(self._impl, 'get_analyzer_status'):
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æå™¨
            info.update(self._impl.get_analyzer_status())
        elif hasattr(self._impl, 'nlp'):
            # ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†æå™¨
            info.update({
                'spacy_available': SPACY_AVAILABLE,
                'ginza_model': str(self._impl.nlp.meta.get('name', 'unknown')) if self._impl.nlp else None,
                'dictionary_size': len(self._impl.sentiment_dict) if hasattr(self._impl, 'sentiment_dict') else 0
            })
        
        return info
    
    def get_metrics(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å°‚ç”¨ï¼‰"""
        if hasattr(self._impl, 'get_metrics'):
            return self._impl.get_metrics()
        else:
            return {'error': 'ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™'}


class LegacySentimentAnalyzer:
    """ginza+spacyã«ã‚ˆã‚‹æ„Ÿæƒ…åˆ†æã‚¯ãƒ©ã‚¹"""
    
    # å‹•çš„é–¾å€¤è¨­å®š
    SENTIMENT_THRESHOLDS = {
        'strong_negative': 20,
        'mild_negative': 40,
        'neutral': 60,
        'mild_positive': 80,
        'strong_positive': 90
    }
    
    # æ–‡è„ˆä¿®é£¾å­ã®é‡ã¿ä»˜ã‘
    CONTEXT_MODIFIERS = {
        'negation': -0.8,
        'intensifier': 1.5,
        'diminisher': 0.6,
        'uncertainty': 0.7
    }
    
    def __init__(self):
        self.nlp: Optional[Language] = None
        self.sentiment_dict: Dict[str, float] = {}
        # SPACYãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿åˆæœŸåŒ–
        if SPACY_AVAILABLE:
            self._initialize_ginza()
            self._load_sentiment_dictionary()
    
    def _initialize_ginza(self) -> None:
        """GiNZAãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹"""
        try:
            self.nlp = spacy.load('ja_ginza_electra')
            logger.info("ja_ginza_electra ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
        except OSError:
            try:
                self.nlp = spacy.load('ja_ginza')
                logger.info("ja_ginza ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
            except OSError:
                logger.error("GiNZAãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ pipã§ginzaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
                raise RuntimeError("GiNZAãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ pipã§ginzaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    def _load_sentiment_dictionary(self) -> None:
        """æ„Ÿæƒ…è¾æ›¸ã‚’èª­ã¿è¾¼ã‚€"""
        # è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®å„ªå…ˆé †ä½: toukou_pn.txt > new_pn_ja.dic > pn_ja.dic
        dict_candidates = [
            ('toukou_pn.txt', 'toukou'),
            ('new_pn_ja.dic', 'pnja'),
            ('pn_ja.dic', 'pnja'),
            ('pn_ja_takamura.dic', 'pnja')
        ]
        
        # è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ãƒ‘ã‚¹
        dict_base_paths = [
            os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'sentiment_dictionaries'),  # é–‹ç™ºç’°å¢ƒ
            os.path.join('/app', 'data', 'sentiment_dictionaries'),  # Dockerç’°å¢ƒ
            os.path.join('/app', '..'),  # æ—§ãƒ‘ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        ]
        
        for dict_file, dict_type in dict_candidates:
            for base_path in dict_base_paths:
                dict_path = os.path.join(base_path, dict_file)
                if os.path.exists(dict_path):
                    logger.info(f"{dict_file} ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ: {dict_path}")
                    self._load_dictionary_file(dict_path, dict_type)
                    return
        
        logger.warning("è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªæ„Ÿæƒ…èªå½™ã®ã¿ä½¿ç”¨ã—ã¾ã™")
        self._load_fallback_dictionary()
    
    def _load_dictionary_file(self, file_path: str, dict_type: str) -> None:
        """è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if dict_type == 'pnja':
            self._load_pnja_dictionary(file_path)
        elif dict_type == 'toukou':
            self._load_toukou_dictionary(file_path)
    
    def _load_pnja_dictionary(self, file_path: str) -> None:
        """pn_ja.dicå½¢å¼ã®è¾æ›¸ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(file_path, encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        parts = line.split('\t')
                        if len(parts) >= 2:
                            word = parts[0]
                            sentiment = parts[1]
                            # ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’è¨­å®š
                            if sentiment == 'p':
                                self.sentiment_dict[word] = 1.0
                            elif sentiment == 'n':
                                self.sentiment_dict[word] = -1.0
                            elif sentiment == 'e':
                                self.sentiment_dict[word] = 0.0
            logger.info(f"pn_ja.dicè¾æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.sentiment_dict)}èª")
        except Exception as e:
            logger.error(f"pn_ja.dicèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self._load_fallback_dictionary()
    
    def _load_toukou_dictionary(self, file_path: str) -> None:
        """æ±å·¥å¤§æ„Ÿæƒ…æ¥µæ€§è¾æ›¸ã‚’èª­ã¿è¾¼ã‚€"""
        encodings = ['utf-8', 'cp932', 'shift_jis']
        
        for encoding in encodings:
            try:
                with open(file_path, encoding=encoding) as f:
                    for line in f:
                        line = line.strip()
                        if line and ':' in line:
                            parts = line.split(':')
                            if len(parts) >= 4:
                                word = parts[0]
                                reading = parts[1]
                                try:
                                    score = float(parts[3])
                                    self.sentiment_dict[word] = score
                                    # èª­ã¿ã‚‚ç™»éŒ²
                                    if reading != '*' and reading != word:
                                        self.sentiment_dict[reading] = score
                                except ValueError:
                                    continue
                logger.info(f"æ±å·¥å¤§è¾æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.sentiment_dict)}èª")
                return
            except UnicodeDecodeError:
                continue
        
        logger.error("æ±å·¥å¤§è¾æ›¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        self._load_fallback_dictionary()
    
    def _load_fallback_dictionary(self) -> None:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬æ„Ÿæƒ…èªå½™"""
        basic_emotions = {
            # ãƒã‚¸ãƒ†ã‚£ãƒ–
            'å¬‰ã—ã„': 1.0, 'ã†ã‚Œã—ã„': 1.0, 'æ¥½ã—ã„': 1.0, 'ãŸã®ã—ã„': 1.0,
            'å¹¸ã›': 1.0, 'ã—ã‚ã‚ã›': 1.0, 'æœ€é«˜': 1.0, 'ç´ æ™´ã‚‰ã—ã„': 1.0,
            'ç¾ã—ã„': 1.0, 'å¯æ„›ã„': 1.0, 'ã‹ã‚ã„ã„': 1.0, 'å¥½ã': 1.0,
            'ã‚ã‚ŠãŒã¨ã†': 1.0, 'æ„Ÿè¬': 1.0, 'æ„›': 1.0, 'å¸Œæœ›': 1.0,
            
            # ãƒã‚¬ãƒ†ã‚£ãƒ–
            'æ‚²ã—ã„': -1.0, 'ã‹ãªã—ã„': -1.0, 'è¾›ã„': -1.0, 'ã¤ã‚‰ã„': -1.0,
            'å›°ã£ãŸ': -1.0, 'å«Œã„': -1.0, 'å«Œ': -1.0, 'æ€’ã‚Š': -1.0,
            'å¿ƒé…': -1.0, 'ä¸å®‰': -1.0, 'å¤±æœ›': -1.0, 'çµ¶æœ›': -1.0,
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«
            'æ™®é€š': 0.0, 'é€šå¸¸': 0.0, 'ã„ã¤ã‚‚': 0.0
        }
        
        self.sentiment_dict.update(basic_emotions)
        logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ã‚’ä½¿ç”¨: {len(basic_emotions)}èª")
    
    def extract_emotion_bearing_tokens(self, doc) -> List[Tuple[str, str, str]]:
        """æ„Ÿæƒ…ã‚’è¡¨ã™å¯èƒ½æ€§ã®ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º"""
        emotion_tokens = []
        
        for sent in doc.sents:
            for token in sent:
                # å“è©ãƒ•ã‚£ãƒ«ã‚¿ã‚’æ‹¡å¼µ
                if token.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV', 'AUX', 'INTJ']:
                    emotion_tokens.append((token.lemma_, token.text, token.pos_))
                
                # æ´»ç”¨å½¢ã‚‚è€ƒæ…®ï¼ˆã€Œé ‘å¼µã‚‹ã€â†’ã€Œé ‘å¼µã‚ã†ã€ï¼‰
                if token.pos_ == 'VERB' and token.text != token.lemma_:
                    emotion_tokens.append((token.text, token.text, 'VERB_CONJUGATED'))
        
        return emotion_tokens
    
    def should_filter_context_dependent(self, word: str, pos: str, sentence: str) -> bool:
        """æ–‡è„ˆä¾å­˜èªå½™ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æ”¹å–„"""
        # é™¤å¤–ã™ã¹ãæ–‡è„ˆä¾å­˜èªï¼ˆæ©Ÿèƒ½èªã‚„ä¸€èˆ¬åè©ï¼‰
        truly_context_dependent = {
            'ã‚ˆã', 'ã¨ã“ã‚', 'ã“ã¨', 'ã‚‚ã®', 'ã»ã‚“ã¨', 'äºº'
        }
        
        # æ„Ÿæƒ…ã«å¯„ä¸ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹èªã¯é™¤å¤–ã—ãªã„
        emotion_contributing = {
            'ä»Šæ—¥', 'æ˜æ—¥', 'æ°—æŒã¡', 'ä¸€æ—¥', 'æ¯æ—¥'
        }
        
        if word in truly_context_dependent:
            return True
        
        if word in emotion_contributing:
            # æ–‡ã®ä¸»è¦ãªå†…å®¹èªã§ã‚ã‚Œã°ä¿æŒ
            return False
        
        return False
    
    def calculate_weighted_score(self, word_scores: List[Tuple[str, float, str]]) -> float:
        """å“è©ã«ã‚ˆã‚‹é‡ã¿ä»˜ãå¹³å‡ã‚’è¨ˆç®—"""
        weights = {
            'ADJ': 1.5,    # å½¢å®¹è©ã¯æ„Ÿæƒ…è¡¨ç¾ã¨ã—ã¦é‡è¦
            'ADV': 1.2,    # å‰¯è©ã‚‚æ„Ÿæƒ…ã®å¼·åº¦ã‚’è¡¨ã™
            'VERB': 1.0,   # å‹•è©ã¯æ¨™æº–
            'NOUN': 0.8,   # åè©ã¯è£œåŠ©çš„
            'PROPN': 0.5,  # å›ºæœ‰åè©ã¯æ„Ÿæƒ…ã¸ã®å¯„ä¸ãŒå°ã•ã„
            'VERB_CONJUGATED': 1.1  # æ´»ç”¨å½¢ã¯æ„Ÿæƒ…è¡¨ç¾ã¨ã—ã¦é‡è¦
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for word, score, pos in word_scores:
            weight = weights.get(pos, 1.0)
            weighted_sum += score * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def integrate_context_score(self, base_score: float, context_score: float) -> float:
        """æ–‡è„ˆã‚¹ã‚³ã‚¢ã‚’é©åˆ‡ã«çµ±åˆ"""
        # æ–‡è„ˆã‚¹ã‚³ã‚¢ã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
        normalized_context = (context_score + 1) / 2  # -1ã€œ1ã‚’0ã€œ1ã«å¤‰æ›
        
        # é‡ã¿ä»˜ãçµåˆï¼ˆæ–‡è„ˆã®å½±éŸ¿ã‚’20%ã«åˆ¶é™ï¼‰
        integrated_score = base_score * 0.8 + normalized_context * 0.2
        
        return integrated_score
    
    def normalize_score(self, raw_score: float, intensity: float = 1.0) -> float:
        """çµ±ä¸€ã•ã‚ŒãŸæ­£è¦åŒ–å‡¦ç†"""
        # -1ã€œ1ã®ç¯„å›²ã‚’æƒ³å®šã—ã€0ã€œ100ã«å¤‰æ›
        # intensityã‚’è€ƒæ…®ã—ãŸéç·šå½¢å¤‰æ›
        normalized = (math.tanh(raw_score * intensity * 2) + 1) / 2
        
        return normalized * 100
    
    def detect_positive_patterns(self, text: str) -> float:
        """æ–‡ç« å…¨ä½“ã®ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        pattern_scores = {
            # åŠ±ã¾ã—ãƒ»æ±ºæ„è¡¨ç¾
            r'(é ‘å¼µ|ãŒã‚“ã°)(ã‚ã†|ã‚Šã¾ã™|ã‚‹|ã£ã¦)': 0.7,
            r'(ç¬‘é¡”|ãˆãŒãŠ)ã§': 0.8,
            r'(æ¥½ã—|ãŸã®ã—)(ã¿|ã‚‚ã†)': 0.8,
            r'(ç´ æ•µ|ã™ã¦ã)ãª': 0.7,
            r'(å¹¸ã›|ã—ã‚ã‚ã›)ã«': 0.8,
            
            # å‰å‘ããªæ„å›³
            r'ã‚ˆã†ã«(é ‘å¼µ|ãŒã‚“ã°|ãªã‚Š|ã™ã‚‹)': 0.6,
            r'(ã§ãã‚‹|å‡ºæ¥ã‚‹)(ã‚ˆã†|ã‚ˆã†ã«)': 0.6,
            
            # æŒ¨æ‹¶ãƒ»ç¥ç¦
            r'(ãŠã¯ã‚ˆã†|ã“ã‚“ã«ã¡ã¯|ã‚ã‚ŠãŒã¨ã†)': 0.5,
            r'(ãŠã‚ã§ã¨ã†|ãŠç–²ã‚Œæ§˜)': 0.6,
        }
        
        total_score = 0.0
        match_count = 0
        
        for pattern, score in pattern_scores.items():
            if re.search(pattern, text):
                total_score += score
                match_count += 1
        
        return total_score / max(match_count, 1)
    
    def calculate_expression_weight(self, text: str) -> float:
        """æ„Ÿå˜†ç¬¦ã‚„çµµæ–‡å­—ã«ã‚ˆã‚‹æ„Ÿæƒ…å¼·åº¦ã®è¨ˆç®—"""
        weight = 1.0
        
        # æ„Ÿå˜†ç¬¦ã®æ•°ã¨ä½ç½®ã‚’è€ƒæ…®
        exclamation_count = text.count('!') + text.count('ï¼')
        if exclamation_count > 0:
            # æ–‡æœ«ã®æ„Ÿå˜†ç¬¦ã¯ã‚ˆã‚Šé‡è¦
            if text.rstrip().endswith(('!', 'ï¼')):
                weight *= (1.0 + 0.3 * min(exclamation_count, 3))
            else:
                weight *= (1.0 + 0.1 * exclamation_count)
        
        # çµµæ–‡å­—ãƒ»é¡”æ–‡å­—ã®æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        positive_emojis = ['ğŸ˜Š', 'ğŸ˜„', 'ğŸ™‚', 'ğŸ‘', 'âœ¨', 'ğŸŒŸ', 'â¤ï¸', '(^_^)', '(^^)']
        for emoji in positive_emojis:
            if emoji in text:
                weight *= 1.2
        
        return weight
    
    def calculate_confidence(self, matched_words: int, total_words: int, text_length: int) -> float:
        """åˆ†æã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        # ãƒãƒƒãƒç‡
        match_rate = matched_words / total_words if total_words > 0 else 0
        
        # ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ã‚ˆã‚‹ä¿¡é ¼åº¦
        length_confidence = min(1.0, text_length / 50)  # 50æ–‡å­—ã§æœ€å¤§ä¿¡é ¼åº¦
        
        # ç·åˆä¿¡é ¼åº¦
        confidence = (match_rate * 0.7 + length_confidence * 0.3)
        
        return round(confidence * 100, 1)
    
    def _get_word_mapping(self) -> Dict[str, List[str]]:
        """è¡¨è¨˜æºã‚Œãƒãƒƒãƒ”ãƒ³ã‚°"""
        return {
            'ã‹ã‚ã„ã„': ['å¯æ„›ã„', 'ã‹ã‚ã„ã‚‰ã—ã„', 'å¯æ„›ã‚‰ã—ã„'],
            'ã†ã‚Œã—ã„': ['å¬‰ã—ã„', 'ã†ã‚Œã—ã„'], 
            'ãŸã®ã—ã„': ['æ¥½ã—ã„', 'ãŸã®ã—ã„'],
            'ã†ã¤ãã—ã„': ['ç¾ã—ã„', 'ã†ã¤ãã—ã„'],
            'ãã‚Œã„': ['ç¶ºéº—', 'ãã‚Œã„', 'å¥‡éº—'],
            'ã™ã¦ã': ['ç´ æ•µ', 'ã™ã¦ã', 'ã‚¹ãƒ†ã‚­'],
            'ã—ã‚ã‚ã›': ['å¹¸ã›', 'ã—ã‚ã‚ã›', 'ã‚·ã‚¢ãƒ¯ã‚»']
        }
    
    def _get_force_positive_words(self) -> Dict[str, float]:
        """å¼·åˆ¶çš„ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ã—ã¦æ‰±ã†èªå½™"""
        return {
            'æœ€é«˜': 1.0, 'ç´ æ™´ã‚‰ã—ã„': 1.0, 'æ„Ÿå‹•': 1.0, 'å¹¸ã›': 1.0,
            'ã—ã‚ã‚ã›': 1.0, 'å¬‰ã—ã„': 1.0, 'ã†ã‚Œã—ã„': 1.0,
            'æ¥½ã—ã„': 1.0, 'ãŸã®ã—ã„': 1.0, 'ç¾ã—ã„': 1.0, 'ç¾å‘³ã—ã„': 1.0
        }
    
    def _apply_context_based_fallback(self, text: str) -> Tuple[float, List[str]]:
        """æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®æ„Ÿæƒ…æ¨å®š"""
        context_score = 0.0
        detected_patterns = []
        
        # ç–‘å•æ–‡ã®æ¤œå‡º
        if '?' in text or 'ï¼Ÿ' in text or re.search(r'[ã ã§ã™ã‹]$', text):
            context_score += 0.2
            detected_patterns.append("ç–‘å•æ–‡â†’surprised")
        
        # æ„Ÿå˜†ç¬¦ã®æ¤œå‡º
        if '!' in text or 'ï¼' in text:
            context_score += 0.4
            detected_patterns.append("æ„Ÿå˜†ç¬¦â†’happy")
        
        # æŒ¨æ‹¶ã®æ¤œå‡º
        greetings = ['ã“ã‚“ã«ã¡ã¯', 'ãŠã¯ã‚ˆã†', 'ã“ã‚“ã°ã‚“ã¯', 'ãŠç–²ã‚Œ', 'ã‚ã‚ŠãŒã¨ã†']
        for greeting in greetings:
            if greeting in text:
                context_score += 0.3
                detected_patterns.append(f"{greeting}â†’happy")
                break
        
        # å¦å®šçš„ãªæ–‡è„ˆ
        negative_patterns = ['å›°ã£ãŸ', 'ã ã‚', 'ãƒ€ãƒ¡', 'ç„¡ç†', 'å«Œ', 'ã‚¤ãƒ¤']
        for pattern in negative_patterns:
            if pattern in text:
                context_score -= 0.4
                detected_patterns.append(f"{pattern}â†’sad")
                break
        
        return context_score, detected_patterns
    
    def analyze(self, text: str) -> Tuple[float, SentimentCategory]:
        """æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œã™ã‚‹"""
        if not SPACY_AVAILABLE or not self.nlp:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæ„Ÿæƒ…æ¨å®š
            context_score, _ = self._apply_context_based_fallback(text)
            sentiment_score = self.normalize_score(context_score)
            
            return sentiment_score, self._score_to_category(sentiment_score)
        
        if not text.strip():
            return 50.0, SentimentCategory.NEUTRAL
        
        doc = self.nlp(text)
        
        #ã€€æ„Ÿæƒ…ãƒˆãƒ¼ã‚¯ãƒ³æŠ½å‡º
        emotion_tokens = self.extract_emotion_bearing_tokens(doc)
        matched_words = []
        
        word_mapping = self._get_word_mapping()
        force_positive = self._get_force_positive_words()
        
        # èªå½™åˆ†æã¨é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢è¨ˆç®—
        for word, surface, pos in emotion_tokens:
            # æ–‡è„ˆä¾å­˜èªå½™ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.should_filter_context_dependent(word, pos, text):
                continue
            
            # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®å–å¾—
            word_score = self._get_word_sentiment_score(
                word, surface, pos, word_mapping, force_positive
            )
            
            if word_score is not None:
                # æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚³ã‚¢èª¿æ•´
                adjusted_score = self._adjust_score_by_context_improved(
                    word_score, word, pos, text
                )
                matched_words.append((word, adjusted_score, pos))
        
        # é‡ã¿ä»˜ãå¹³å‡ã«ã‚ˆã‚‹åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
        if matched_words:
            base_score = self.calculate_weighted_score(matched_words)
        else:
            base_score = 0.0
        
        # æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®è£œå®Œã¨ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        context_score, _ = self._apply_context_based_fallback(text)
        positive_patterns = self.detect_positive_patterns(text)
        
        # æ–‡è„ˆã‚¹ã‚³ã‚¢ã®çµ±åˆ
        integrated_score = self.integrate_context_score(base_score, context_score)
        integrated_score += positive_patterns * 0.5  # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
        
        # è¡¨ç¾é‡ã¿ã®é©ç”¨
        expression_weight = self.calculate_expression_weight(text)
        integrated_score *= expression_weight
        
        # çµ±ä¸€ã•ã‚ŒãŸæ­£è¦åŒ–å‡¦ç†
        sentiment_score = self.normalize_score(integrated_score)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
        category = self._score_to_category(sentiment_score)
        
        return sentiment_score, category
    
    def _get_word_sentiment_score(
        self, 
        word: str, 
        surface: str, 
        pos: str,
        word_mapping: Dict[str, List[str]],
        force_positive: Dict[str, float]
    ) -> Optional[float]:
        """å˜èªã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã™ã‚‹"""
        # å¼·åˆ¶ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚§ãƒƒã‚¯
        if word in force_positive:
            return force_positive[word]
        if surface in force_positive:
            return force_positive[surface]
        
        # è¾æ›¸ãƒãƒƒãƒãƒ³ã‚°
        if word in self.sentiment_dict:
            return self.sentiment_dict[word]
        if surface in self.sentiment_dict:
            return self.sentiment_dict[surface]
        
        # è¡¨è¨˜æºã‚Œãƒãƒƒãƒ”ãƒ³ã‚°
        for base_word, variants in word_mapping.items():
            if word == base_word or word in variants:
                for variant in variants:
                    if variant in self.sentiment_dict:
                        return self.sentiment_dict[variant]
        
        # å½¢å®¹è©ã®èªå¹¹ãƒãƒƒãƒãƒ³ã‚°
        if pos == 'ADJ' and word.endswith('ã„'):
            stem = word[:-1]
            for suffix in ['', 'ã•', 'ã¿', 'ã‚‰ã—ã•']:
                candidate = stem + suffix
                if candidate in self.sentiment_dict:
                    return self.sentiment_dict[candidate]
        
        return None
    
    def _adjust_score_by_context_improved(self, score: float, word: str, pos: str, 
                                        sentence_context: str = "") -> float:
        """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸã‚¹ã‚³ã‚¢èª¿æ•´ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        # çœŸã«æ–‡è„ˆä¾å­˜çš„ã§æ„Ÿæƒ…ä¾¡å€¤ãŒä½ã„èªã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        truly_neutral_words = {'ã¨ã“ã‚', 'ã“ã¨', 'ã‚‚ã®', 'ã‚ˆã', 'ã¿ã‚‹'}
        
        if word in truly_neutral_words and abs(score) < 0.3:
            return 0.0
        
        # æ–‡è„ˆä¿®é£¾å­ã®é©ç”¨
        modifier = self._detect_context_modifier(sentence_context, word)
        if modifier:
            score *= self.CONTEXT_MODIFIERS[modifier]
        
        # ã‚¹ã‚³ã‚¢èª¿æ•´ã‚’ç·©å’Œ
        if score < 0:
            # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¹ã‚³ã‚¢ã®éåº¦ãªæŠ‘åˆ¶ã‚’é¿ã‘ã‚‹
            if score >= -0.3:
                return score * 0.5  # 0.2 â†’ 0.5ã«ç·©å’Œ
            elif score >= -0.7:
                return score * 0.7  # 0.5 â†’ 0.7ã«ç·©å’Œ
        elif score > 0:
            # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚¹ã‚³ã‚¢ã®é©åˆ‡ãªå¼·åŒ–
            if pos in ['ADJ', 'VERB', 'INTJ']:  # æ„Ÿæƒ…è¡¨ç¾ã¨ã—ã¦é‡è¦ãªå“è©
                return score * 1.2
        
        return score
    
    def _adjust_score_by_context(self, score: float, word: str, pos: str, sentence_context: str = "") -> float:
        """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸã‚¹ã‚³ã‚¢èª¿æ•´"""
        return self._adjust_score_by_context_improved(score, word, pos, sentence_context)
    
    def _detect_context_modifier(self, sentence_context: str, word: str) -> Optional[str]:
        """æ–‡è„ˆä¿®é£¾å­ã‚’æ¤œå‡ºã™ã‚‹"""
        # å¦å®šèªã®æ¤œå‡º
        negation_words = ['ãªã„', 'ã§ã¯ãªã„', 'ã§ã¯ã‚ã‚Šã¾ã›ã‚“', 'ããªã„', 'ãšã«']
        for neg_word in negation_words:
            if neg_word in sentence_context:
                return 'negation'
        
        # å¼·èª¿èªã®æ¤œå‡º
        intensifier_words = ['ã¨ã¦ã‚‚', 'éå¸¸ã«', 'è¶…', 'ã‚ã¡ã‚ƒ', 'ã™ã”ã', 'æœ¬å½“ã«', 'å®Ÿã«', 'ã‹ãªã‚Š']
        for int_word in intensifier_words:
            if int_word in sentence_context:
                return 'intensifier'
        
        # å¼±åŒ–èªã®æ¤œå‡º
        diminisher_words = ['å°‘ã—', 'ã¡ã‚‡ã£ã¨', 'ã‚„ã‚„', 'ã‚ãšã‹ã«', 'ã¾ã‚ã¾ã‚', 'ãã“ãã“']
        for dim_word in diminisher_words:
            if dim_word in sentence_context:
                return 'diminisher'
        
        # ä¸ç¢ºå®Ÿæ€§è¡¨ç¾ã®æ¤œå‡º
        uncertainty_words = ['ãŸã¶ã‚“', 'ãŠãã‚‰ã', 'ã‚‚ã—ã‹ã—ãŸã‚‰', 'ãªã‚“ã¨ãªã', 'ã‚ˆã†ãªæ°—ãŒã™ã‚‹']
        for unc_word in uncertainty_words:
            if unc_word in sentence_context:
                return 'uncertainty'
        
        return None
    
    # æ—§ç‰ˆãƒ¡ã‚½ãƒƒãƒ‰ã¯ä¸‹ä½äº’æ›æ€§ã®ãŸã‚ä¿æŒ
    def normalize_score_sigmoid(self, raw_score: float) -> float:
        """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’ä½¿ç”¨ã—ãŸéç·šå½¢æ­£è¦åŒ–"""
        sigmoid_score = 1 / (1 + math.exp(-raw_score * 3))
        return sigmoid_score * 100
    
    def normalize_score_tanh(self, raw_score: float) -> float:
        """åŒæ›²ç·šæ­£æ¥ã‚’ä½¿ç”¨ã—ãŸæ­£è¦åŒ–ï¼ˆã‚ˆã‚Šæ»‘ã‚‰ã‹ãªå¤‰æ›ï¼‰"""
        tanh_score = (math.tanh(raw_score * 2) + 1) / 2
        return tanh_score * 100
    
    def adjust_intensity(self, base_score: float, intensity_factor: float) -> float:
        """æ„Ÿæƒ…å¼·åº¦ã«åŸºã¥ãã‚¹ã‚³ã‚¢èª¿æ•´"""
        if base_score > 50:  # ãƒã‚¸ãƒ†ã‚£ãƒ–
            return base_score + (100 - base_score) * intensity_factor * 0.3
        else:  # ãƒã‚¬ãƒ†ã‚£ãƒ–
            return base_score - base_score * intensity_factor * 0.3
    
    def statistical_correction(self, raw_score: float, text_length: int) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé•·ã¨éå»ã®åˆ†æçµæœã‚’è€ƒæ…®ã—ãŸè£œæ­£"""
        length_factor = min(1.0, text_length / 100)  # é•·æ–‡ã»ã©ä¿¡é ¼åº¦UP
        corrected_score = raw_score * (0.7 + 0.3 * length_factor)
        return max(0.0, min(100.0, corrected_score))
    
    def _calculate_intensity_factor(self, text: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ„Ÿæƒ…å¼·åº¦ã‚’è¨ˆç®—"""
        intensity = 0.0
        
        # æ„Ÿå˜†ç¬¦ã€ç–‘å•ç¬¦ã®æ•°
        intensity += (text.count('!') + text.count('ï¼')) * 0.2
        intensity += (text.count('?') + text.count('ï¼Ÿ')) * 0.1
        
        # å¼·èª¿è¡¨ç¾
        intensifiers = ['ã¨ã¦ã‚‚', 'éå¸¸ã«', 'è¶…', 'ã‚ã¡ã‚ƒ', 'ã™ã”ã', 'æœ¬å½“ã«', 'å®Ÿã«']
        for intensifier in intensifiers:
            if intensifier in text:
                intensity += 0.3
        
        # å¼±åŒ–è¡¨ç¾
        diminishers = ['å°‘ã—', 'ã¡ã‚‡ã£ã¨', 'ã‚„ã‚„', 'ã‚ãšã‹ã«', 'ã¾ã‚ã¾ã‚']
        for diminisher in diminishers:
            if diminisher in text:
                intensity -= 0.2
        
        return max(0.0, min(1.0, intensity))
    
    def _score_to_category(self, score: float) -> SentimentCategory:
        """å‹•çš„é–¾å€¤ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚³ã‚¢ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹"""
        if score >= self.SENTIMENT_THRESHOLDS['strong_positive']:
            return SentimentCategory.STRONG_POSITIVE
        elif score >= self.SENTIMENT_THRESHOLDS['mild_positive']:
            return SentimentCategory.MILD_POSITIVE
        elif score >= self.SENTIMENT_THRESHOLDS['neutral']:
            return SentimentCategory.NEUTRAL
        elif score >= self.SENTIMENT_THRESHOLDS['mild_negative']:
            return SentimentCategory.MILD_NEGATIVE
        else:
            return SentimentCategory.STRONG_NEGATIVE 